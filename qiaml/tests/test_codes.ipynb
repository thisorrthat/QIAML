{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "764be0ee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "#from autocrop import crop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5c62b112",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv2\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def crop(path=None):\n",
    "    \"\"\"\n",
    "    Returns a cropped version of the image with the path path.\n",
    "\n",
    "    Parameters:\n",
    "    - path: path to the image being cropped\n",
    "\n",
    "    Returns:\n",
    "    - cropped: image which is the cropped version of the image with the path\n",
    "               path\n",
    "    \"\"\"\n",
    "    assert type(path) == str, 'The path should be in string format!'\n",
    "    \n",
    "    img = cv2.imread(path, cv2.IMREAD_COLOR)\n",
    "    \n",
    "    assert np.sum(img) != None, 'the path is not right or there is no such a file. Check path or file name.'\n",
    "    \n",
    "    assert img.shape[0:3] != None, 'The image is not in right format. Image should have three diamensions'\n",
    "\n",
    "    assert img.shape[0] >100 and img.shape[1] >100, 'the image is too blurred. Please retake or reload'\n",
    "    \n",
    "    \n",
    "    # leave only green color\n",
    "    img[:, :, 0] = 0\n",
    "    img[:, :, 2] = 0\n",
    "\n",
    "    # convert to gray scale\n",
    "    gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # more contrast between foreground and background\n",
    "    contrasted_img = apply_contrast(gray_img)\n",
    "\n",
    "    # erode image\n",
    "    kernel = np.ones((5, 5), np.uint8)\n",
    "    eroded_img = cv2.erode(contrasted_img, kernel, iterations=15)\n",
    "\n",
    "    # located contours\n",
    "    contours = _locate_contours(eroded_img)\n",
    "\n",
    "    img_contours = np.zeros(img.shape)\n",
    "    cv2.drawContours(img_contours, contours, -1, (0, 255, 0), 3)\n",
    "\n",
    "    # determine cropped image based on contours\n",
    "    crop_box = _determine_cropped_image_box(img, contours)\n",
    "\n",
    "    # crop the original image\n",
    "    cropped = img[crop_box[1]:crop_box[3], crop_box[0]:crop_box[2]]\n",
    "\n",
    "    return cropped\n",
    "\n",
    "\n",
    "def apply_contrast(img):\n",
    "    \"\"\"\n",
    "    Returns a contrasted version of img.\n",
    "\n",
    "    Returns:\n",
    "    - contrasted_img: contrasted version of img\n",
    "    \"\"\"\n",
    "\n",
    "    contrast_threshold = 2\n",
    "    grid_size = 2\n",
    "    alpha = 3  # (1.0-3.0)\n",
    "    beta = 0  # (0-100)\n",
    "\n",
    "    # CLAHE (Contrast Limited Adaptive Histogram Equalization)\n",
    "    clahe = cv2.createCLAHE(clipLimit=contrast_threshold,\n",
    "                            tileGridSize=(grid_size, grid_size))\n",
    "    clahe_img = clahe.apply(img)\n",
    "\n",
    "    adjusted = cv2.convertScaleAbs(clahe_img, alpha=alpha, beta=beta)\n",
    "\n",
    "    return adjusted\n",
    "\n",
    "\n",
    "def _locate_contours(img):\n",
    "    \"\"\"\n",
    "    Returns the substantial contours in img.\n",
    "\n",
    "    Parameters:\n",
    "    - img: the image being analyzed\n",
    "\n",
    "    Returns:\n",
    "    - substantial_contours: the substantial contours in img\n",
    "    \"\"\"\n",
    "\n",
    "    min_threshold = 75\n",
    "    threshold_output = 255\n",
    "    min_countour_area = 15000\n",
    "\n",
    "    _, threshold = cv2.threshold(img, min_threshold,\n",
    "                                 threshold_output,\n",
    "                                 cv2.THRESH_BINARY)\n",
    "\n",
    "    # dilated = cv2.morphologyEx(threshold, cv2.MORPH_OPEN,\n",
    "    #                            cv2.getStructuringElement(cv2.MORPH_ELLIPSE,\n",
    "    #                            (10, 10)))\n",
    "\n",
    "    contours, _ = cv2.findContours(threshold, cv2.RETR_LIST,\n",
    "                                   cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    substantial_contours = []\n",
    "    for contour in contours:\n",
    "        if cv2.contourArea(contour) > min_countour_area:\n",
    "            substantial_contours.append(contour)\n",
    "\n",
    "    cv2.drawContours(img, contours, -1, (0, 255, 0), 3)\n",
    "\n",
    "    return substantial_contours\n",
    "\n",
    "\n",
    "def _determine_cropped_image_box(img, contours):\n",
    "    \"\"\"\n",
    "    Returns the pixel box including all contours in contours.\n",
    "\n",
    "    Parameters:\n",
    "    - img: the image being analyzed\n",
    "    - contours: a list of countours thats locations should be included in the\n",
    "                outputted box size\n",
    "\n",
    "    Returns:\n",
    "    - crop_box: the pixel box including all contours in contours in form [left,\n",
    "                top, right, bottom]\n",
    "    \"\"\"\n",
    "    # https://stackoverflow.com/questions/37803903/opencv-and-python-for-auto-cropping\n",
    "    crop_box = [-1, -1, -1, -1]\n",
    "    for contour in contours:\n",
    "        contour_x, contour_y, contour_w, contour_h = cv2.boundingRect(contour)\n",
    "        if crop_box[0] < 0:\n",
    "            crop_box = [contour_x, contour_y, contour_x + contour_w,\n",
    "                        contour_y + contour_h]\n",
    "        elif contour_x > np.shape(img)[0] / 2:\n",
    "            crop_box[0] = min(contour_x, crop_box[0])\n",
    "            crop_box[1] = min(contour_y, crop_box[1])\n",
    "            crop_box[2] = max(contour_x + contour_w, crop_box[2])\n",
    "            crop_box[3] = max(contour_y + contour_h, crop_box[3])\n",
    "    \n",
    "    # add bounding space\n",
    "    crop_box[0] = max(0, crop_box[0] - 50)\n",
    "    crop_box[1] = max(0, crop_box[1] - 50)\n",
    "    crop_box[2] = min(np.shape(img)[0], crop_box[2] + 100)\n",
    "    crop_box[3] = min(np.shape(img)[1], crop_box[3] + 100)\n",
    "\n",
    "    return crop_box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5bed9c2f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type('test_image.jpg') == str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "da18b279",
   "metadata": {},
   "outputs": [],
   "source": [
    "testpath = 'test_image1.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aaa76fae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "The path should be in string format!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_193/44903529.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_193/1648238580.py\u001b[0m in \u001b[0;36mcrop\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     14\u001b[0m                \u001b[0mpath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \"\"\"\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0;32massert\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'The path should be in string format!'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIMREAD_COLOR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: The path should be in string format!"
     ]
    }
   ],
   "source": [
    "dat = crop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "725c320c",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "The path should be in string format!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_193/3946130383.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_193/1648238580.py\u001b[0m in \u001b[0;36mcrop\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     14\u001b[0m                \u001b[0mpath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \"\"\"\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0;32massert\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'The path should be in string format!'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIMREAD_COLOR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: The path should be in string format!"
     ]
    }
   ],
   "source": [
    "crop(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb05b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c5cdeabc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dat = crop('test_image.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8ea98ddd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "img = cv2.imread('test_image.jpg', cv2.IMREAD_COLOR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "61a4c829",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(859, 861, 3)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7cfab77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert 2000> img.shape[0] >100 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "57bff7b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2000> img.shape[0] >100 and 2000> img.shape[1] >100 and img.shape[2] ==3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3492ec8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "img.shape[0:3] != None and sum(img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "94f9683e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img.ndim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "52b36b71",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "img = cv2.imread('test_image2.jpg', cv2.IMREAD_COLOR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "05a16f24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(142, 355, 3)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae04ebe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0963682",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f2de3436",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img.any == None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3f465b4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(859, 861, 3)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img.shape[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e635f6be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img.shape[0:3] != None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e80df218",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "861"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "956297f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img.shape[0] >100 and img.shape[1] >100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cb06351a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img.shape[1] >100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "78253885",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img.shape[0] >100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba158674",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Wed Feb 23 21:52:41 2022\n",
    "\n",
    "@author: Colem\n",
    "\n",
    "Modified on Sun Mar 13\n",
    "assertions wrote for tests\n",
    "Xuetao.\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Data sets All and AB are two part needing to be concatenated, all other data\n",
    "sets are one piece \n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "def get_data_array(Dataset=None):\n",
    "    \"\"\"\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    Dataset : str. Which data set desired: A,B,C,AB,AC,BC\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    data : array \n",
    "        Intexting the first image, array[i][j]-> i is the number of arrays in\n",
    "        range of number of images, j=0 is eelecting image array, j=1 is the\n",
    "        input copy number .\n",
    "        \n",
    "    \"\"\"\n",
    "    assert type(Dataset) == str, 'The chosen dataset names should be in string format!'\n",
    "    \n",
    "    Dataset_choices = ['All','AB','A','B','C','AC','BC']\n",
    "    assert Dataset in Dataset_choices, 'Error: Datasets limited to A, B, C, AB, AC, BC'\n",
    "    path = './Datasets/'\n",
    "    ispath = os.path.isdir(path)\n",
    "    assert ispath, 'Error Datasets is not downloaded into directory. Please download Datasets.'\n",
    "    \n",
    "    \n",
    "    if Dataset == 'All':\n",
    "        All_1 = np.load('./Datasets/QIAML_All_1_Data.npy', allow_pickle=True)\n",
    "        All_2 = np.load('./Datasets/QIAML_All_2_Data.npy', allow_pickle=True)\n",
    "        data = np.concatenate((All_1, All_2), axis=0)\n",
    "        return data\n",
    "    elif Dataset == 'AB':\n",
    "        All_1 = np.load('./Datasets/QIAML_Data_AB_1.npy', allow_pickle=True)\n",
    "        All_2 = np.load('./Datasets/QIAML_Data_AB_2.npy', allow_pickle=True)\n",
    "        data = np.concatenate((All_1, All_2), axis=0)\n",
    "        return data\n",
    "    elif Dataset == 'A':\n",
    "        data = np.load('./Datasets/QIAML_Data_A.npy', allow_pickle=True)\n",
    "        return data\n",
    "    elif Dataset == 'B':\n",
    "        data = np.load('./Datasets/QIAML_Data_B.npy', allow_pickle=True)\n",
    "        return data\n",
    "    elif Dataset == 'C':\n",
    "        data = np.load('./Datasets/QIAML_Data_C.npy', allow_pickle=True)\n",
    "        return data\n",
    "    elif Dataset == 'AC':\n",
    "        All_1 = np.load('./Datasets/QIAML_Data_AC_1.npy', allow_pickle=True)\n",
    "        All_2 = np.load('./Datasets/QIAML_Data_AC_2.npy', allow_pickle=True)\n",
    "        data = np.concatenate((All_1, All_2), axis=0)\n",
    "        return data\n",
    "    elif Dataset == 'BC':\n",
    "        data = np.load('./Datasets/QIAML_Data_BC.npy', allow_pickle=True)\n",
    "        return data\n",
    "\n",
    "def get_data_df(Dataset):\n",
    "    \"\"\"\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    Dataset : str. Which data set desired: A,B,C,AB,AC,BC\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    data : pandas dataframe. \n",
    "        DESCRIPTION: dataframe with columns: images, Copy number\n",
    "\n",
    "    \"\"\"\n",
    "    if Dataset == 'All':\n",
    "        data = pd.DataFrame(get_data_array(Dataset), columns = ['Images', 'Copy number'])\n",
    "        return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17032f29",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Error Datasets is not downloaded into directory. Please download Datasets.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_193/259944237.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_data_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'All'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_193/1651955336.py\u001b[0m in \u001b[0;36mget_data_array\u001b[0;34m(Dataset)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'./Datasets/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0mispath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m     \u001b[0;32massert\u001b[0m \u001b[0mispath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Error Datasets is not downloaded into directory. Please download Datasets.'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: Error Datasets is not downloaded into directory. Please download Datasets."
     ]
    }
   ],
   "source": [
    "get_data_array('All')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a9378474",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './Datasets/'\n",
    "path2 = '__pycache__'\n",
    "isdir = os.path.isdir(path2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f3e5da8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ff22f320",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./Datasets/'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1551fb4c",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "The chosen dataset names should be in string format!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_193/446197869.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_data_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_193/1651955336.py\u001b[0m in \u001b[0;36mget_data_array\u001b[0;34m(Dataset)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \"\"\"\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0;32massert\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'The chosen dataset names should be in string format!'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0mDataset_choices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'All'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'AB'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'A'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'B'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'C'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'AC'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'BC'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: The chosen dataset names should be in string format!"
     ]
    }
   ],
   "source": [
    "get_data_array()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5676ff92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "None == str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef807484",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
